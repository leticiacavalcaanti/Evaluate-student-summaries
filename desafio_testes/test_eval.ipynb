{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:21<00:00,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import backoff\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "deepseek_api = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "client = OpenAI(api_key=deepseek_api, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "CACHE_FILE = 'prediction_cache.json'\n",
    "BATCH_SIZE = 50  # Increased batch size\n",
    "\n",
    "def load_cache():\n",
    "    if Path(CACHE_FILE).exists():\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(cache, f)\n",
    "\n",
    "def create_prompt(row):\n",
    "    # ... existing prompt creation code ...\n",
    "    return f\"\"\"Analyze this unit test and determine if it's a good test case.\n",
    "    \n",
    "Function being tested:\n",
    "{row['function_python_code']}\n",
    "\n",
    "Original function call:\n",
    "- Call: {row['function_string_call']}\n",
    "- Parameters: {row['function_call_parameters']}\n",
    "- Output: {row['function_call_output']}\n",
    "\n",
    "Unit test:\n",
    "- Test parameters: {row['unit_test_parameters']}\n",
    "- Expected output: {row['unit_test_output']}\n",
    "- Assertion: {row['unit_test_assertion']}\n",
    "\n",
    "Is this a good test case? Consider:\n",
    "1. Do the test parameters match the function's requirements?\n",
    "2. Is the expected output correct for these parameters?\n",
    "3. Does the assertion correctly validate the function?\n",
    "\n",
    "Respond with only 1 (good test) or 0 (bad test).\n",
    "\"\"\"\n",
    "\n",
    "@backoff.on_exception(backoff.expo, \n",
    "                     (aiohttp.ClientError, asyncio.TimeoutError),\n",
    "                     max_tries=5)\n",
    "async def get_llm_prediction_async(prompt, session):\n",
    "    try:\n",
    "        async with session.post(\n",
    "            \"https://api.deepseek.com/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": \"deepseek-chat\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 1\n",
    "            },\n",
    "            headers={\"Authorization\": f\"Bearer {deepseek_api}\"}\n",
    "        ) as response:\n",
    "            result = await response.json()\n",
    "            return int(result['choices'][0]['message']['content'].strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting prediction: {e}\")\n",
    "        return 0\n",
    "\n",
    "async def process_batch_async(df, cache):\n",
    "    predictions = []\n",
    "    uncached_rows = []\n",
    "    row_indices = []\n",
    "    \n",
    "    # Check cache first\n",
    "    for idx, row in df.iterrows():\n",
    "        cache_key = f\"{row['function_string_call']}_{row['unit_test_parameters']}\"\n",
    "        if cache_key in cache:\n",
    "            predictions.append(cache[cache_key])\n",
    "        else:\n",
    "            uncached_rows.append(row)\n",
    "            row_indices.append(idx)\n",
    "    \n",
    "    if uncached_rows:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for row in uncached_rows:\n",
    "                prompt = create_prompt(row)\n",
    "                tasks.append(get_llm_prediction_async(prompt, session))\n",
    "            \n",
    "            # Process uncached rows concurrently\n",
    "            uncached_predictions = await asyncio.gather(*tasks)\n",
    "            \n",
    "            # Update cache with new predictions\n",
    "            for i, pred in enumerate(uncached_predictions):\n",
    "                cache_key = f\"{uncached_rows[i]['function_string_call']}_{uncached_rows[i]['unit_test_parameters']}\"\n",
    "                cache[cache_key] = pred\n",
    "                predictions.append(pred)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "async def main_async():\n",
    "    # Load data and cache\n",
    "    train_df = pd.read_csv('python-code-unit-test-assertion-quality-prediction/train.csv')\n",
    "    test_df = pd.read_csv('python-code-unit-test-assertion-quality-prediction/test.csv')\n",
    "    cache = load_cache()\n",
    "    \n",
    "    print(\"Processing test data...\")\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Process in larger batches\n",
    "    for i in tqdm(range(0, len(test_df), BATCH_SIZE)):\n",
    "        batch = test_df.iloc[i:i + BATCH_SIZE]\n",
    "        predictions = await process_batch_async(batch, cache)\n",
    "        all_predictions.extend(predictions)\n",
    "        \n",
    "        # Save cache periodically\n",
    "        if i % (BATCH_SIZE * 5) == 0:\n",
    "            save_cache(cache)\n",
    "    \n",
    "    # Final cache save\n",
    "    save_cache(cache)\n",
    "    \n",
    "    # Create submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'y_pred_unit_test_parameters_match': all_predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission saved!\")\n",
    "\n",
    "# For Jupyter notebook execution\n",
    "await main_async()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
